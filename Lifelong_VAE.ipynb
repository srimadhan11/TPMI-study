{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "employed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "scheduled-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import pprint\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invalid-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.student_teacher import StudentTeacher\n",
    "from models.vae.parallelly_reparameterized_vae import ParallellyReparameterizedVAE\n",
    "from models.vae.sequentially_reparameterized_vae import SequentiallyReparameterizedVAE\n",
    "\n",
    "from datasets.loader import get_split_data_loaders, get_loader\n",
    "\n",
    "from helpers.fid import train_fid_model\n",
    "from helpers.layers import EarlyStopping\n",
    "from helpers.grapher import Grapher\n",
    "from helpers.metrics import calculate_consistency, calculate_fid, estimate_fisher\n",
    "\n",
    "from helpers.utils import dummy_context, number_of_parameters\n",
    "from helpers.utils import float_type, ones_like, append_to_csv\n",
    "from helpers.utils import num_samples_in_loader, check_or_create_dir\n",
    "\n",
    "from optimizers.adamnormgrad import AdamNormGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lucky-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_obj():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpful-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_map = {\n",
    "    \"rmsprop\" : optim.RMSprop,\n",
    "    \"adam\"    : optim.Adam,\n",
    "    \"adamnorm\": AdamNormGrad,\n",
    "    \"adadelta\": optim.Adadelta,\n",
    "    \"sgd\"     : optim.SGD,\n",
    "    \"lbfgs\"   : optim.LBFGS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-conservation",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "executed-understanding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--no-cuda'], dest='no_cuda', nargs=0, const=True, default=False, type=None, choices=None, help='disables CUDA training', metavar=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='LifeLong VAE Pytorch')\n",
    "\n",
    "# Task parameters\n",
    "parser.add_argument('--uid'                 , type=str, default=\"\"                           , help=\"add a custom task-specific unique id; appended to name\")\n",
    "parser.add_argument('--task'                , type=str, default=\"mnist\"                      , help='task to work on (can specify multiple) [mnist+cifar10+fashion+svhn_centered+svhn+clutter+permuted]')\n",
    "parser.add_argument('--epochs'              , type=int, default=10             , metavar='N' , help='minimum number of epochs to train')\n",
    "parser.add_argument('--continuous-size'     , type=int, default=32             , metavar='L' , help='latent size of continuous variable when using mixture or gaussian')\n",
    "parser.add_argument('--discrete-size'       , type=int, default=1                            , help='initial dim of discrete variable when using mixture or gumbel')\n",
    "parser.add_argument('--download'            , type=int, default=1                            , help='download dataset from s3')\n",
    "parser.add_argument('--data-dir'            , type=str, default='./.datasets'  , metavar='DD', help='directory which contains input data')\n",
    "parser.add_argument('--output-dir'          , type=str, default='./experiments', metavar='OD', help='directory which contains csv results')\n",
    "parser.add_argument('--model-dir'           , type=str, default='.models'      , metavar='MD', help='directory which contains trained models')\n",
    "parser.add_argument('--fid-model-dir'       , type=str, default='.models'                    , help='directory which contains trained FID models')\n",
    "parser.add_argument('--calculate-fid-with'  , type=str, default=None                         , help='enables FID calc & uses model conv/inceptionv3')\n",
    "parser.add_argument('--disable-augmentation', action='store_true'                            , help='disables student-teacher data augmentation')\n",
    "\n",
    "# train / eval or resume modes\n",
    "parser.add_argument('--resume-training-with', type=int, default=None, help='tries to load the model from model_dir and resume training')\n",
    "parser.add_argument('--eval-with'           , type=int, default=None, help='tries to load the model from model_dir and evaluate the test dataset')\n",
    "parser.add_argument('--eval-with-loader'    , type=int, default=None, help='if there are many loaders use ONLY this loader')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--filter-depth'           , type=int, default=32                  , help='number of initial conv filter maps')\n",
    "parser.add_argument('--reparam-type'           , type=str, default='isotropic_gaussian', help='isotropic_gaussian, discrete or mixture')\n",
    "parser.add_argument('--layer-type'             , type=str, default='conv'              , help='dense or conv')\n",
    "parser.add_argument('--nll-type'               , type=str, default='bernoulli'         , help='bernoulli or gaussian')\n",
    "parser.add_argument('--vae-type'               , type=str, default='parallel'          , help='vae type [sequential or parallel]')\n",
    "parser.add_argument('--normalization'          , type=str, default='groupnorm'         , help='normalization type: batchnorm/groupnorm/instancenorm/none')\n",
    "parser.add_argument('--activation'             , type=str, default='elu'               , help='activation function')\n",
    "parser.add_argument('--log-interval'           , type=int, default=10     , metavar='N', help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--disable-sequential'     , action='store_true', help='enables standard batch training')\n",
    "parser.add_argument('--shuffle-minibatches'    , action='store_true', help='shuffles the student\\'s minibatch')\n",
    "parser.add_argument('--use-relational-encoder' , action='store_true', help='uses a relational network as the encoder projection layer')\n",
    "parser.add_argument('--use-pixel-cnn-decoder'  , action='store_true', help='uses a pixel CNN decoder')\n",
    "parser.add_argument('--disable-gated-conv'     , action='store_true', help='disables gated convolutional structure')\n",
    "parser.add_argument('--disable-student-teacher', action='store_true', help='uses a standard VAE without Student-Teacher architecture')\n",
    "\n",
    "# Optimization related\n",
    "parser.add_argument('--early-stop', action='store_true', help='enable early stopping')\n",
    "parser.add_argument('--optimizer' , type=str  , default=\"adamnorm\", choices=optim_map.keys(), help=\"specify optimizer\")\n",
    "parser.add_argument('--lr'        , type=float, default=1e-3      , metavar='LR'            , help='learning rate')\n",
    "parser.add_argument('--batch-size', type=int  , default=64        , metavar='N'             , help='input batch size for training')\n",
    "\n",
    "# Regularizer related\n",
    "parser.add_argument('--disable-regularizers', action='store_true', help='disables mutual info and consistency regularizers')\n",
    "parser.add_argument('--monte-carlo-infogain', action='store_true', help='use the MC version of mutual information gain / false is analytic')\n",
    "parser.add_argument('--continuous-mut-info' , type=float, default=0.0    , help='-continuous_mut_info * I(z_c; x) is applied (opposite dir of disc)')\n",
    "parser.add_argument('--discrete-mut-info'   , type=float, default=0.0    , help='+discrete_mut_info * I(z_d; x) is applied')\n",
    "parser.add_argument('--kl-reg'              , type=float, default=1.0    , help='hyperparameter to scale KL term in ELBO')\n",
    "parser.add_argument('--generative-scale-var', type=float, default=1.0    , help='scale variance of prior in order to capture outliers')\n",
    "parser.add_argument('--consistency-gamma'   , type=float, default=1.0    , help='consistency_gamma * KL(Q_student | Q_teacher)')\n",
    "parser.add_argument('--likelihood-gamma'    , type=float, default=0.0    , help='log-likelihood regularizer between teacher and student, 0 is disabled')\n",
    "parser.add_argument('--mut-clamp-strategy'  , type=str  , default=\"clamp\", help='clamp mut info by norm / clamp / none')\n",
    "parser.add_argument('--mut-clamp-value'     , type=float, default=100.0  , help='max / min clamp value if above strategy is clamp')\n",
    "parser.add_argument('--ewc-gamma'           , type=float, default=0      , help='any value greater than 0 enables EWC with this hyper-parameter')\n",
    "\n",
    "# Visdom parameters\n",
    "parser.add_argument('--visdom-url' , type=str, default=\"localhost\", help='visdom URL for graphs')\n",
    "parser.add_argument('--visdom-port', type=int, default=\"9001\"     , help='visdom port for graphs')\n",
    "\n",
    "# Device parameters\n",
    "parser.add_argument('--seed'   , type=int           , default=None , help='seed for numpy and pytorch')\n",
    "parser.add_argument('--ngpu'   , type=int           , default=1    , help='number of gpus available')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='disables CUDA training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "relevant-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = '''--batch-size=300 --reparam-type=mixture --discrete-size=100 --continuous-size=40 --epochs=2 --layer-type=conv --ngpu=1 --optimizer=adam --disable-regularizers --disable-sequential --task=mnist --uid=mnistfashionpermuted --calculate-fid-with=conv'''\n",
    "#argv = '''--batch-size=300 --reparam-type=mixture --discrete-size=100 --continuous-size=40 --epochs=200 --layer-type=conv --ngpu=1 --optimizer=adam --disable-regularizers --disable-sequential --task=mnist+fashion+permuted --uid=mnistfashionpermuted --calculate-fid-with=conv'''\n",
    "#argv = '''--batch-size=300 --reparam-type=mixture --discrete-size=100 --continuous-size=40 --epochs=50 --layer-type=conv --ngpu=4 --optimizer=adam --disable-regularizers --disable-sequential --task=mnist+cifar10+fashion+svhn_centered+svhn+clutter+permuted --uid=mnistfashion --calculate-fid-with=conv'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "prime-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(argv.split())\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "\n",
    "# handle randomness / non-randomness\n",
    "if args.seed:\n",
    "    print(\"I: Setting seed {}\".format(args.seed))\n",
    "    numpy.random.seed(args.seed)\n",
    "    torch.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "manual-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'activation': 'elu',\n",
      "    'batch_size': 300,\n",
      "    'calculate_fid_with': 'conv',\n",
      "    'consistency_gamma': 1.0,\n",
      "    'continuous_mut_info': 0.0,\n",
      "    'continuous_size': 40,\n",
      "    'cuda': True,\n",
      "    'data_dir': './.datasets',\n",
      "    'disable_augmentation': False,\n",
      "    'disable_gated_conv': False,\n",
      "    'disable_regularizers': True,\n",
      "    'disable_sequential': True,\n",
      "    'disable_student_teacher': False,\n",
      "    'discrete_mut_info': 0.0,\n",
      "    'discrete_size': 100,\n",
      "    'download': 1,\n",
      "    'early_stop': False,\n",
      "    'epochs': 2,\n",
      "    'eval_with': None,\n",
      "    'eval_with_loader': None,\n",
      "    'ewc_gamma': 0,\n",
      "    'fid_model_dir': '.models',\n",
      "    'filter_depth': 32,\n",
      "    'generative_scale_var': 1.0,\n",
      "    'kl_reg': 1.0,\n",
      "    'layer_type': 'conv',\n",
      "    'likelihood_gamma': 0.0,\n",
      "    'log_interval': 10,\n",
      "    'lr': 0.001,\n",
      "    'model_dir': '.models',\n",
      "    'monte_carlo_infogain': False,\n",
      "    'mut_clamp_strategy': 'clamp',\n",
      "    'mut_clamp_value': 100.0,\n",
      "    'ngpu': 1,\n",
      "    'nll_type': 'bernoulli',\n",
      "    'no_cuda': False,\n",
      "    'normalization': 'groupnorm',\n",
      "    'optimizer': 'adam',\n",
      "    'output_dir': './experiments',\n",
      "    'reparam_type': 'mixture',\n",
      "    'resume_training_with': None,\n",
      "    'seed': None,\n",
      "    'shuffle_minibatches': False,\n",
      "    'task': 'mnist',\n",
      "    'uid': 'mnistfashionpermuted',\n",
      "    'use_pixel_cnn_decoder': False,\n",
      "    'use_relational_encoder': False,\n",
      "    'vae_type': 'parallel',\n",
      "    'visdom_port': 9001,\n",
      "    'visdom_url': 'localhost'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(args.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dressed-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(model):\n",
    "    return optim_map[args.optimizer](model.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "def register_plots(loss, grapher, epoch, prefix='train'):\n",
    "    for k, v in loss.items():\n",
    "        if isinstance(v, map):\n",
    "            register_plots(loss[k], grapher, epoch, prefix=prefix)\n",
    "\n",
    "        if 'mean' in k or 'scalar' in k:\n",
    "            key_name = k.split('_')[0]\n",
    "            value = v if isinstance(v, (float, np.float32, np.float64)) else v.item()\n",
    "            grapher.register_single({'%s_%s' % (prefix, key_name): [[epoch], [value]]}, plot_type='line')\n",
    "\n",
    "\n",
    "def register_images(images, names, grapher, prefix=\"train\"):\n",
    "    ''' helper to register a list of images '''\n",
    "    if isinstance(images, list):\n",
    "        assert len(images) == len(names)\n",
    "        for im, name in zip(images, names):\n",
    "            register_images(im, name, grapher, prefix=prefix)\n",
    "    else:\n",
    "        images = torch.min(images.detach(), ones_like(images))\n",
    "        grapher.register_single({'{}_{}'.format(prefix, names): images}, plot_type='imgs')\n",
    "\n",
    "\n",
    "def _add_loss_map(loss_tm1, loss_t):\n",
    "    if not loss_tm1: # base case: empty dict\n",
    "        resultant = {'count': 1}\n",
    "        for k, v in loss_t.items():\n",
    "            if 'mean' in k or 'scalar' in k:\n",
    "                resultant[k] = v.detach()\n",
    "        return resultant\n",
    "\n",
    "    resultant = {}\n",
    "    for (k, v) in loss_t.items():\n",
    "        if 'mean' in k or 'scalar' in k:\n",
    "            resultant[k] = loss_tm1[k] + v.detach()\n",
    "\n",
    "    # increment total count\n",
    "    resultant['count'] = loss_tm1['count'] + 1\n",
    "    return resultant\n",
    "\n",
    "\n",
    "def _mean_map(loss_map):\n",
    "    for k in loss_map.keys():\n",
    "        loss_map[k] /= loss_map['count']\n",
    "    return loss_map\n",
    "\n",
    "\n",
    "def train(epoch, model, fisher, optimizer, loader, grapher, prefix='train'):\n",
    "    return execute_graph(epoch=epoch, model=model, fisher=fisher, data_loader=loader,\n",
    "                         grapher=grapher, optimizer=optimizer, prefix='train')\n",
    "\n",
    "\n",
    "def test(epoch, model, fisher, loader, grapher, prefix='test'):\n",
    "    return execute_graph(epoch, model=model, fisher=fisher, data_loader=loader,\n",
    "                          grapher=grapher, optimizer=None, prefix='test')\n",
    "\n",
    "\n",
    "def execute_graph(epoch, model, fisher, data_loader, grapher, optimizer=None, prefix='test'):\n",
    "    ''' execute the graph; when 'train' is in the name the model runs the optimizer '''\n",
    "    loss_map, params, num_samples, is_train = {}, {}, 0, prefix == 'train'\n",
    "    \n",
    "    if is_train:\n",
    "        model.train()\n",
    "        assert optimizer is not None\n",
    "    else:\n",
    "        model.eval()\n",
    "        assert optimizer is None\n",
    "    \n",
    "\n",
    "    for data, _ in data_loader:\n",
    "        set_trace()\n",
    "        data = Variable(data).cuda() if args.cuda else Variable(data)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad() if not is_train else dummy_context():    # run the VAE and extract loss\n",
    "            output_map = model(data)\n",
    "            loss_t = model.loss_function(output_map, fisher)\n",
    "\n",
    "        if is_train:\n",
    "            loss_t['loss_mean'].backward()\n",
    "            loss_t['grad_norm_mean'] = torch.norm(nn.utils.parameters_to_vector(model.parameters()))\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad() if not is_train else dummy_context():\n",
    "            loss_map = _add_loss_map(loss_map, loss_t)\n",
    "            num_samples += data.size(0)\n",
    "\n",
    "    loss_map = _mean_map(loss_map) # reduce the map to get actual means\n",
    "    print('{:5s}[Epoch {:4d}][{:7d} samples]:\\tAvgLoss: {:.4f}\\tELBO: {:.4f}\\tKLD: {:.4f}\\tNLL: {:.4f}\\tMut: {:.4f}'.format(\n",
    "        prefix, epoch, num_samples,\n",
    "        loss_map['loss_mean'].item(), loss_map['elbo_mean'].item(),\n",
    "        loss_map[ 'kld_mean'].item(), loss_map[ 'nll_mean'].item(),\n",
    "        loss_map['mut_info_mean'].item())\n",
    "    )\n",
    "\n",
    "    # gather scalar values of reparameterizers (if they exist)\n",
    "    reparam_scalars = model.student.get_reparameterizer_scalars()\n",
    "\n",
    "    # plot the test accuracy, loss and images\n",
    "    if grapher:\n",
    "        register_plots({**loss_map, **reparam_scalars}, grapher, epoch=epoch, prefix=prefix)\n",
    "        images = [output_map['augmented']['data'], output_map['student']['x_reconstr']]\n",
    "        img_names = ['original_imgs', 'vae_reconstructions']\n",
    "        register_images(images, img_names, grapher, prefix=prefix)\n",
    "        grapher.show()\n",
    "\n",
    "    # return this for early stopping\n",
    "    loss_val = {\n",
    "        'loss_mean': loss_map['loss_mean'].detach().item(),\n",
    "        'elbo_mean': loss_map['elbo_mean'].detach().item()\n",
    "    }\n",
    "    loss_map.clear()\n",
    "    params.clear()\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "def generate(student_teacher, grapher, name='teacher'):\n",
    "    model = {\n",
    "        'teacher': student_teacher.teacher,\n",
    "        'student': student_teacher.student\n",
    "    }\n",
    "\n",
    "    if model[name] is not None: # handle base case\n",
    "        model[name].eval()\n",
    "        gen = student_teacher.generate_synthetic_samples(model[name], args.batch_size)    # random generation\n",
    "        gen = torch.min(gen, ones_like(gen))\n",
    "        grapher.register_single({'generated_%s'%name: gen}, plot_type='imgs')\n",
    "\n",
    "        # sequential generation for discrete and mixture reparameterizations\n",
    "        if args.reparam_type in ('mixture', 'discrete'):\n",
    "            gen = student_teacher.generate_synthetic_sequential_samples(model[name]).detach()\n",
    "            gen = torch.min(gen, ones_like(gen))\n",
    "            grapher.register_single({'sequential_generated_%s'%name: gen}, plot_type='imgs')\n",
    "\n",
    "\n",
    "def get_model_and_loader():\n",
    "    ''' helper to return the model and the loader '''\n",
    "    if args.disable_sequential: # vanilla batch training\n",
    "        loaders = get_loader(args)\n",
    "        loaders = loaders if isinstance(loaders, list) else [loaders]\n",
    "    else:                       # classes split\n",
    "        loaders = get_split_data_loaders(args, num_classes=10)\n",
    "    for l in loaders:\n",
    "        print(\"train = {:7d}\\t| test = {:7d}\".format(\n",
    "            num_samples_in_loader(l.train_loader), num_samples_in_loader(l.test_loader))\n",
    "        )\n",
    "\n",
    "    # append the image shape to the config & build the VAE\n",
    "    args.img_shp =  loaders[0].img_shp,\n",
    "    if args.vae_type == 'sequential':\n",
    "        # Sequential : P(y|x) --> P(z|y, x) --> P(x|z)\n",
    "        # Keep a separate VAE spawn here in case we want to parameterize the sequence of reparameterizers\n",
    "        vae = SequentiallyReparameterizedVAE(loaders[0].img_shp, kwargs=vars(args))\n",
    "    elif args.vae_type == 'parallel':\n",
    "        # Ours: [P(y|x), P(z|x)] --> P(x | z)\n",
    "        vae = ParallellyReparameterizedVAE(loaders[0].img_shp, kwargs=vars(args))\n",
    "    else:\n",
    "        raise Exception(\"unknown VAE type requested\")\n",
    "\n",
    "    # build the combiner which takes in the VAE as a parameter and projects the latent representation to the output space\n",
    "    student_teacher = StudentTeacher(vae, kwargs=vars(args))\n",
    "\n",
    "    grapher = Grapher(env=student_teacher.get_name(), server=args.visdom_url, port=args.visdom_port)\n",
    "    return student_teacher, loaders, grapher\n",
    "\n",
    "\n",
    "def lazy_generate_modules(model, img_shp):\n",
    "    ''' Super hax, but needed for building lazy modules '''\n",
    "    model.eval()\n",
    "    data = float_type(args.cuda)(args.batch_size, *img_shp).normal_()\n",
    "    model(Variable(data))\n",
    "\n",
    "\n",
    "def test_and_generate(epoch, model, fisher, loader, grapher):\n",
    "    test_loss = test(epoch=epoch, model=model, fisher=fisher,\n",
    "                     loader=loader.test_loader, grapher=grapher, prefix='test')\n",
    "    generate(model, grapher, 'student') # generate student samples\n",
    "    generate(model, grapher, 'teacher') # generate teacher samples\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def eval_model(data_loaders, model, fid_model, args):\n",
    "    ''' simple helper to evaluate the model over all the loaders'''\n",
    "    for loader in data_loaders:\n",
    "        test_loss = test(epoch=-1, model=model, fisher=None,\n",
    "                         loader=loader.test_loader, grapher=None, prefix='test')\n",
    "\n",
    "        # evaluate and save away one-time metrics\n",
    "        check_or_create_dir(os.path.join(args.output_dir))\n",
    "        append_to_csv([test_loss['elbo_mean']], os.path.join(args.output_dir, \"{}_test_elbo.csv\".format(args.uid)))\n",
    "        append_to_csv(calculate_consistency(model, loader, args.reparam_type, args.vae_type, args.cuda),\n",
    "                      os.path.join(args.output_dir, \"{}_consistency.csv\".format(args.uid)))\n",
    "        with open(os.path.join(args.output_dir, \"{}_conf.json\".format(args.uid)), 'w') as f:\n",
    "            json.dump(model.student.config, f)\n",
    "\n",
    "        if args.calculate_fid_with is not None:\n",
    "            # TODO: parameterize num fid samples, currently use less for inceptionv3 as it's COSTLY\n",
    "            num_fid_samples = 4000 if args.calculate_fid_with != 'inceptionv3' else 1000\n",
    "            append_to_csv(calculate_fid(fid_model=fid_model, model=model, loader=loader,\n",
    "                                        grapher=None, num_samples=num_fid_samples, cuda=args.cuda),\n",
    "                          os.path.join(args.output_dir, \"{}_fid.csv\".format(args.uid)))\n",
    "\n",
    "\n",
    "def train_loop(data_loaders, model, fid_model, grapher, args):\n",
    "    ''' simple helper to run the entire train loop; not needed for eval modes'''\n",
    "    optimizer = build_optimizer(model.student)     # collect our optimizer\n",
    "    print(\"there are {} params with {} elems in the st-model and {} params in the student with {} elems\".format(\n",
    "        len(list(model.parameters())), number_of_parameters(model),\n",
    "        len(list(model.student.parameters())), number_of_parameters(model.student))\n",
    "    )\n",
    "\n",
    "    # main training loop\n",
    "    fisher = None\n",
    "    for j, loader in enumerate(data_loaders):\n",
    "        num_epochs = args.epochs # TODO: randomize epochs by something like: + np.random.randint(0, 13)\n",
    "        print(\"training current distribution for {} epochs\".format(num_epochs))\n",
    "        early = EarlyStopping(model, max_steps=50, burn_in_interval=None) if args.early_stop else None\n",
    "\n",
    "        test_loss = None\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train(epoch, model, fisher, optimizer, loader.train_loader, grapher)\n",
    "            test_loss = test(epoch, model, fisher, loader.test_loader, grapher)\n",
    "            if args.early_stop and early(test_loss['loss_mean']):\n",
    "                early.restore()                 # restore and test+generate again\n",
    "                test_loss = test_and_generate(epoch, model, fisher, loader, grapher)\n",
    "                break\n",
    "\n",
    "            generate(model, grapher, 'student') # generate student samples\n",
    "            generate(model, grapher, 'teacher') # generate teacher samples\n",
    "\n",
    "        '''\n",
    "        evaluate and save away one-time metrics, these include:\n",
    "            1. test elbo\n",
    "            2. FID\n",
    "            3. consistency\n",
    "            4. num synth + num true samples\n",
    "            5. dump config to visdom\n",
    "        '''\n",
    "        \n",
    "        check_or_create_dir(os.path.join(args.output_dir))\n",
    "        append_to_csv([test_loss['elbo_mean']], os.path.join(args.output_dir, \"{}_test_elbo.csv\".format(args.uid)))\n",
    "        append_to_csv([test_loss['elbo_mean']], os.path.join(args.output_dir, \"{}_test_elbo.csv\".format(args.uid)))\n",
    "        \n",
    "        num_synth_samples = np.ceil(epoch * args.batch_size * model.ratio)\n",
    "        num_true_samples  = np.ceil(epoch * (args.batch_size - (args.batch_size * model.ratio)))\n",
    "        \n",
    "        append_to_csv([num_synth_samples], os.path.join(args.output_dir, \"{}_numsynth.csv\".format(args.uid)))\n",
    "        append_to_csv([num_true_samples ], os.path.join(args.output_dir, \"{}_numtrue.csv\".format(args.uid)))\n",
    "        append_to_csv([epoch            ], os.path.join(args.output_dir, \"{}_epochs.csv\".format(args.uid)))\n",
    "        \n",
    "        grapher.vis.text(num_synth_samples, opts=dict(title=\"num_synthetic_samples\"))\n",
    "        grapher.vis.text(num_true_samples , opts=dict(title=\"num_true_samples\"))\n",
    "        grapher.vis.text(pprint.PrettyPrinter(indent=4).pformat(model.student.config), opts=dict(title=\"config\"))\n",
    "\n",
    "        if j > 0:   # calc the consistency using the **PREVIOUS** loader\n",
    "            append_to_csv(calculate_consistency(model, data_loaders[j - 1], args.reparam_type, args.vae_type, args.cuda),\n",
    "                          os.path.join(args.output_dir, \"{}_consistency.csv\".format(args.uid)))\n",
    "\n",
    "\n",
    "        if args.calculate_fid_with is not None:\n",
    "            # TODO: parameterize num fid samples, currently use less for inceptionv3 as it's COSTLY\n",
    "            num_fid_samples = 4000 if args.calculate_fid_with != 'inceptionv3' else 1000\n",
    "            append_to_csv(calculate_fid(fid_model=fid_model, model=model, loader=loader, grapher=grapher,\n",
    "                                        num_samples=num_fid_samples, cuda=args.cuda),\n",
    "                          os.path.join(args.output_dir, \"{}_fid.csv\".format(args.uid)))\n",
    "\n",
    "        grapher.save() # save the remote visdom graphs\n",
    "        if j != len(data_loaders) - 1:\n",
    "            if args.ewc_gamma > 0:\n",
    "                # calculate the fisher from the previous data loader\n",
    "                print(\"computing fisher info matrix....\")\n",
    "                fisher_tmp = estimate_fisher(model.student, # this is pre-fork\n",
    "                                             loader, args.batch_size, cuda=args.cuda)\n",
    "                if fisher is not None:\n",
    "                    assert len(fisher) == len(fisher_tmp), \"#fisher params != #new fisher params\"\n",
    "                    for (kf, vf), (kft, vft) in zip(fisher.items(), fisher_tmp.items()):\n",
    "                        fisher[kf] += fisher_tmp[kft]\n",
    "                else:\n",
    "                    fisher = fisher_tmp\n",
    "\n",
    "            # spawn a new student & rebuild grapher; we also pass\n",
    "            # the new model's parameters through a new optimizer.\n",
    "            if not args.disable_student_teacher:\n",
    "                model.fork()\n",
    "                lazy_generate_modules(model, data_loaders[0].img_shp)\n",
    "                optimizer = build_optimizer(model.student)\n",
    "                print(\"there are {} params with {} elems in the st-model and {} params in the student with {} elems\".format(\n",
    "                    len(list(model.parameters()))        , number_of_parameters(model),\n",
    "                    len(list(model.student.parameters())), number_of_parameters(model.student))\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # increment anyway for vanilla models, so that we can have a separate visdom env\n",
    "                model.current_model += 1\n",
    "\n",
    "            grapher = Grapher(env=model.get_name(), server=args.visdom_url, port=args.visdom_port)\n",
    "\n",
    "\n",
    "def _set_model_indices(model, grapher, idx, args):\n",
    "    def _init_vae(img_shp, config):\n",
    "        if args.vae_type == 'sequential':\n",
    "            # Sequential : P(y|x) --> P(z|y, x) --> P(x|z)\n",
    "            # Keep a separate VAE spawn here in case we want to parameterize the sequence of reparameterizers\n",
    "            vae = SequentiallyReparameterizedVAE(img_shp, **{'kwargs': config})\n",
    "        elif args.vae_type == 'parallel':\n",
    "            # Ours: [P(y|x), P(z|x)] --> P(x | z)\n",
    "            vae = ParallellyReparameterizedVAE(img_shp, **{'kwargs': config})\n",
    "        else:\n",
    "            raise Exception(\"unknown VAE type requested\")\n",
    "\n",
    "        return vae\n",
    "\n",
    "    if idx > 0:         # create some clean models to later load in params\n",
    "        model.current_model = idx\n",
    "        if not args.disable_augmentation:\n",
    "            model.ratio         = idx / (idx + 1.0)\n",
    "            num_teacher_samples = int(args.batch_size * model.ratio)\n",
    "            num_student_samples = max(args.batch_size - num_teacher_samples, 1)\n",
    "            print(\"teacher_samples: {:7d} | student_samples: {:7d}\".format(num_teacher_samples, num_student_samples))\n",
    "\n",
    "            # copy args and reinit clean models for student and teacher\n",
    "            config_base    = vars(args)\n",
    "            config_teacher = deepcopy(config_base)\n",
    "            config_student = deepcopy(config_base)\n",
    "            config_teacher['discrete_size'] += idx - 1\n",
    "            config_student['discrete_size'] += idx\n",
    "            model.student  = _init_vae(model.student.input_shape, config_student)\n",
    "            if not args.disable_student_teacher:\n",
    "                model.teacher = _init_vae(model.student.input_shape, config_teacher)\n",
    "\n",
    "        # re-init grapher\n",
    "        grapher = Grapher(env=model.get_name(), server=args.visdom_url, port=args.visdom_port)\n",
    "\n",
    "    return model, grapher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "corrected-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    # collect our model and data loader\n",
    "    model, data_loaders, grapher = get_model_and_loader()\n",
    "\n",
    "    # since some modules are lazy generated, we want to run a single fwd pass\n",
    "    lazy_generate_modules(model, data_loaders[0].img_shp)\n",
    "\n",
    "    # build a classifier to use for FID\n",
    "    fid_model = None\n",
    "    if args.calculate_fid_with is not None:\n",
    "        fid_batch_size = args.batch_size if args.calculate_fid_with == 'conv' else 32\n",
    "        fid_model      = train_fid_model(args, args.calculate_fid_with, fid_batch_size)\n",
    "\n",
    "    # handle logic on whether to start /resume training or to eval\n",
    "    if args.eval_with is None and args.resume_training_with is None:              # normal train loop\n",
    "        print(\"starting main training loop from scratch...\")\n",
    "        train_loop(data_loaders, model, fid_model, grapher, args)\n",
    "    elif args.eval_with is None and args.resume_training_with is not None:    # resume training from latest model\n",
    "        print(\"resuming training on model {}...\".format(args.resume_training_with))\n",
    "        model, grapher = _set_model_indices(model, grapher, args.resume_training_with, args)\n",
    "        lazy_generate_modules(model, data_loaders[0].img_shp)\n",
    "        if not model.load(): # restore after setting model ind\n",
    "            raise Exception(\"model failed to load for resume training...\")\n",
    "\n",
    "        train_loop(data_loaders[args.resume_training_with:], model, fid_model, grapher, args)\n",
    "    elif args.eval_with is not None:                                      # eval the provided model\n",
    "        print(\"evaluating model {}...\".format(args.eval_with))\n",
    "        model, grapher = _set_model_indices(model, grapher, args.eval_with, args)\n",
    "        lazy_generate_modules(model, data_loaders[0].img_shp)\n",
    "        if not model.load(): # restore after setting model ind\n",
    "            raise Exception(\"model failed to load for resume training...\")\n",
    "\n",
    "        if args.eval_with_loader is not None: # only use 1 loader\n",
    "            eval_model([data_loaders[args.eval_with_loader]], model, fid_model, args)\n",
    "        else:\n",
    "            eval_model(data_loaders, model, fid_model, args)\n",
    "    else:\n",
    "        raise Exception(\"unknown train-eval-resume combo specified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "forbidden-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train =   60000\t| test =   10000\n",
      "using mixture reparameterizer\n",
      "building gated conv encoder...\n",
      "building gated conv decoder...\n",
      "[FID] train = 60000 | test = 10000 | output_classes = 10\n",
      "compiling standard convnet FID model\n",
      "loading existing FID model\n",
      "\n",
      "[FID 9900 samples]Test Epoch: -1\tAverage loss: 0.0348\tAverage Accuracy: 0.9884\n",
      "\n",
      "starting main training loop from scratch...\n",
      "there are 72 params with 8658681 elems in the st-model and 72 params in the student with 8658681 elems\n",
      "training current distribution for 2 epochs\n",
      "> \u001b[0;32m<ipython-input-50-0ce286edb397>\u001b[0m(75)\u001b[0;36mexecute_graph\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     73 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 75 \u001b[0;31m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     77 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> data\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "ipdb> data.shape\n",
      "torch.Size([300, 1, 28, 28])\n",
      "ipdb> data_loader\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f4b01b71198>\n",
      "ipdb> data.dtype\n",
      "torch.float32\n",
      "ipdb> data[0][0]\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373,\n",
      "         0.3333, 0.5529, 0.9922, 0.9961, 0.9922, 0.2235, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.6588, 0.6627, 0.6588, 0.9922,\n",
      "         0.9843, 0.9922, 0.9843, 0.9922, 0.9843, 0.6588, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5529, 0.9922, 0.9961, 0.9922, 0.9961, 0.9922, 0.9961,\n",
      "         0.9922, 0.9961, 0.9922, 0.9961, 0.7686, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.9922, 0.9843, 0.9922, 0.9843, 0.9922, 0.9843, 0.9922,\n",
      "         0.9843, 0.7686, 0.3255, 0.3294, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.9922, 0.8863, 0.6588, 0.6627, 0.2196, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2235, 0.6588, 0.9922, 0.9843, 0.8824, 0.2196, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5529, 0.9922, 1.0000, 0.9922, 0.9961, 0.9922, 0.7765, 0.3333, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.9922, 0.9843, 0.9922, 0.9843, 0.9922, 0.9843, 0.9922, 0.9843, 0.4471,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7765, 0.9922, 0.8863, 0.6588, 0.6627, 0.6588, 0.9961, 0.9922, 0.9961,\n",
      "         0.5451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1098, 0.3255, 0.2196, 0.0000, 0.0000, 0.0000, 0.5451, 0.9843, 0.9922,\n",
      "         0.9843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.8824, 0.9961,\n",
      "         0.9922, 0.6627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588, 0.9922,\n",
      "         0.9843, 0.6588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9922, 0.9961,\n",
      "         0.9922, 0.4471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9843, 0.9922,\n",
      "         0.9843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 0.7725, 0.7765, 0.1098,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9922, 0.9961,\n",
      "         0.7686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9843, 0.9922, 0.3255,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7725, 0.9843, 0.7686,\n",
      "         0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373, 0.9922, 0.9961, 0.9922,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 0.9922, 0.9961, 0.9922, 0.6627,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9843, 0.9922, 0.9843,\n",
      "         0.4471, 0.0000, 0.2235, 0.6588, 0.9922, 0.9843, 0.9922, 0.9843, 0.2196,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 0.9922,\n",
      "         0.9961, 0.9922, 0.9961, 0.9922, 0.9961, 0.9922, 0.8863, 0.2196, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.3255,\n",
      "         0.9922, 0.9843, 0.9922, 0.9843, 0.9922, 0.5412, 0.2196, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fb32ecbe7ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-dad0f44c4ac1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_with\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_training_with\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m              \u001b[0;31m# normal train loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starting main training loop from scratch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrapher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_with\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_training_with\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# resume training from latest model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming training on model {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_training_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0ce286edb397>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(data_loaders, model, fid_model, grapher, args)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrapher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrapher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mearly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0ce286edb397>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, fisher, optimizer, loader, grapher, prefix)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfisher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrapher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     return execute_graph(epoch=epoch, model=model, fisher=fisher, data_loader=loader,\n\u001b[0;32m---> 53\u001b[0;31m                          grapher=grapher, optimizer=optimizer, prefix='train')\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0ce286edb397>\u001b[0m in \u001b[0;36mexecute_graph\u001b[0;34m(epoch, model, fisher, data_loader, grapher, optimizer, prefix)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0ce286edb397>\u001b[0m in \u001b[0;36mexecute_graph\u001b[0;34m(epoch, model, fisher, data_loader, grapher, optimizer, prefix)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/madhan/miniconda3/envs/pmi/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/madhan/miniconda3/envs/pmi/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tensor_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extraordinary-dover",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# to reload a module ap100 gpufter changing it\n",
    "# import importlib\n",
    "# importlib.reload(packagename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afraid-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-design",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
